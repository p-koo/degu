{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfbb9f8e-0144-499a-abcf-4974f4a06591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:12:52.341045: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 13:12:52.521076: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-27 13:12:52.585047: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-27 13:12:53.209160: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/mosaic/miniconda3/envs/tf2/lib/\n",
      "2024-05-27 13:12:53.209221: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/mosaic/miniconda3/envs/tf2/lib/\n",
      "2024-05-27 13:12:53.209225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os, h5py\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45949ce1-2311-437e-b118-60f1d4c38f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402296, 249, 4)\n",
      "(41186, 2)\n"
     ]
    }
   ],
   "source": [
    "filepath = 'deepstarr_data.h5'\n",
    "dataset = h5py.File(filepath, 'r')\n",
    "x_train = np.array(dataset['x_train']).astype(np.float32)\n",
    "y_train = np.array(dataset['y_train']).astype(np.float32).transpose()\n",
    "x_valid = np.array(dataset['x_valid']).astype(np.float32)\n",
    "y_valid = np.array(dataset['y_valid']).astype(np.float32).transpose()\n",
    "x_test = np.array(dataset['x_test']).astype(np.float32)\n",
    "y_test = np.array(dataset['y_test']).astype(np.float32).transpose()\n",
    "\n",
    "# get shapes\n",
    "N, L, A = x_valid.shape\n",
    "num_labels = 2\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde84aa5-7dda-4003-baf6-673037529c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian_nll_loss(y_true, y_pred):\n",
    "    mean = tf.expand_dims(y_pred[:,0], axis=1)\n",
    "    log_variance = tf.expand_dims(y_pred[:,1], axis=1)\n",
    "\n",
    "    # Calculate the negative log-likelihood\n",
    "    mse = keras.losses.mean_squared_error(y_true, mean)\n",
    "    variance = tf.exp(log_variance)\n",
    "    nll = 0.5 * (tf.math.log(2 * np.pi * variance) + mse / variance)\n",
    "\n",
    "    # Return the average NLL across the batch\n",
    "    return tf.reduce_mean(nll)\n",
    "    \n",
    "\n",
    "def laplace_nll_loss(y_true, y_pred):\n",
    "    mu = tf.expand_dims(y_pred[:,0], axis=1)\n",
    "    log_b = tf.expand_dims(y_pred[:,1], axis=1)\n",
    "\n",
    "    # Calculate the absolute error\n",
    "    abs_error = tf.abs(y_true - mu)\n",
    "\n",
    "    # Calculate the negative log-likelihood\n",
    "    b = tf.exp(log_b)\n",
    "    nll = abs_error / b + log_b + tf.math.log(2.0)\n",
    "\n",
    "    # Return the average NLL across the batch\n",
    "    return tf.reduce_mean(nll)\n",
    "\n",
    "\n",
    "def cauchy_nll_loss(y_true, y_pred):\n",
    "    mu = tf.expand_dims(y_pred[:,0], axis=1)\n",
    "    log_b = tf.expand_dims(y_pred[:,1], axis=1)\n",
    "\n",
    "    # Calculate the negative log-likelihood\n",
    "    b = tf.exp(log_b)\n",
    "    nll = tf.math.log(np.pi * b) + tf.math.log(1 + tf.square((y_true - mu) / b))\n",
    "\n",
    "    # Return the average NLL across the batch\n",
    "    return tf.reduce_mean(nll)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a863d5a0-2793-4bc9-84f7-8137ec243052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DeepSTARR(input_shape):\n",
    "\n",
    "  inputs = keras.layers.Input(shape=input_shape)\n",
    "  x = keras.layers.Conv1D(256, kernel_size=7, padding='same')(inputs)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = keras.layers.Activation('relu')(x)\n",
    "  x = keras.layers.MaxPooling1D(2)(x)\n",
    "\n",
    "  x = keras.layers.Conv1D(60, kernel_size=3, padding='same')(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = keras.layers.Activation('relu')(x)\n",
    "  x = keras.layers.MaxPooling1D(2)(x)\n",
    "\n",
    "  x = keras.layers.Conv1D(60, kernel_size=5, padding='same')(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = keras.layers.Activation('relu')(x)\n",
    "  x = keras.layers.MaxPooling1D(2)(x)\n",
    "\n",
    "  x = keras.layers.Conv1D(120, kernel_size=3, padding='same')(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = keras.layers.Activation('relu')(x)\n",
    "  x = keras.layers.MaxPooling1D(2)(x)\n",
    "\n",
    "  x = keras.layers.Flatten()(x)\n",
    "\n",
    "  x = keras.layers.Dense(256)(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = keras.layers.Activation('relu')(x)\n",
    "  x = keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "  x = keras.layers.Dense(256)(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = keras.layers.Activation('relu')(x)\n",
    "  x = keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "  task1 = keras.layers.Dense(2, activation='linear', name='task1')(x)\n",
    "  # task1 = GaussianLayer()(task1)\n",
    "\n",
    "\n",
    "  task2 = keras.layers.Dense(2, activation='linear', name='task2')(x)\n",
    "  # task2 = GaussianLayer()(task2)\n",
    "\n",
    "  outputs = [task1, task2]\n",
    "\n",
    "  return inputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e01b68-9fb5-4f10-8cf1-a4ffb8868e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 11:26:03.832681: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2024-05-27 11:26:04.627485: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-27 11:26:04.627933: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-27 11:26:04.627940: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-27 11:26:04.628384: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-27 11:26:04.628411: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-05-27 11:26:05.282960: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3143/3143 [==============================] - 50s 15ms/step - loss: 3.7051 - task1_loss: 1.8546 - task2_loss: 1.8505 - val_loss: 3.5104 - val_task1_loss: 1.7560 - val_task2_loss: 1.7544 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3143/3143 [==============================] - 51s 16ms/step - loss: 3.4263 - task1_loss: 1.7132 - task2_loss: 1.7132 - val_loss: 3.6887 - val_task1_loss: 1.8431 - val_task2_loss: 1.8456 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3143/3143 [==============================] - 47s 15ms/step - loss: 3.3732 - task1_loss: 1.6866 - task2_loss: 1.6866 - val_loss: 3.3402 - val_task1_loss: 1.6702 - val_task2_loss: 1.6701 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3143/3143 [==============================] - 49s 15ms/step - loss: 3.3360 - task1_loss: 1.6680 - task2_loss: 1.6680 - val_loss: 3.3207 - val_task1_loss: 1.6605 - val_task2_loss: 1.6602 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3143/3143 [==============================] - 47s 15ms/step - loss: 3.3095 - task1_loss: 1.6547 - task2_loss: 1.6547 - val_loss: 3.2846 - val_task1_loss: 1.6423 - val_task2_loss: 1.6423 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3143/3143 [==============================] - 49s 16ms/step - loss: 3.2877 - task1_loss: 1.6439 - task2_loss: 1.6439 - val_loss: 3.2831 - val_task1_loss: 1.6415 - val_task2_loss: 1.6415 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "3143/3143 [==============================] - 49s 16ms/step - loss: 3.2702 - task1_loss: 1.6351 - task2_loss: 1.6351 - val_loss: 3.2600 - val_task1_loss: 1.6301 - val_task2_loss: 1.6300 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "3143/3143 [==============================] - 49s 16ms/step - loss: 3.2547 - task1_loss: 1.6273 - task2_loss: 1.6273 - val_loss: 3.3771 - val_task1_loss: 1.6889 - val_task2_loss: 1.6882 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "3143/3143 [==============================] - 45s 14ms/step - loss: 3.2397 - task1_loss: 1.6199 - task2_loss: 1.6199 - val_loss: 3.2930 - val_task1_loss: 1.6465 - val_task2_loss: 1.6465 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "3140/3143 [============================>.] - ETA: 0s - loss: 3.2261 - task1_loss: 1.6131 - task2_loss: 1.6131\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "3143/3143 [==============================] - 48s 15ms/step - loss: 3.2261 - task1_loss: 1.6130 - task2_loss: 1.6130 - val_loss: 3.2640 - val_task1_loss: 1.6320 - val_task2_loss: 1.6321 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "3143/3143 [==============================] - 48s 15ms/step - loss: 3.1779 - task1_loss: 1.5890 - task2_loss: 1.5890 - val_loss: 3.2575 - val_task1_loss: 1.6287 - val_task2_loss: 1.6287 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "3143/3143 [==============================] - 48s 15ms/step - loss: 3.1600 - task1_loss: 1.5800 - task2_loss: 1.5800 - val_loss: 3.2732 - val_task1_loss: 1.6366 - val_task2_loss: 1.6365 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "3143/3143 [==============================] - 48s 15ms/step - loss: 3.1506 - task1_loss: 1.5753 - task2_loss: 1.5753 - val_loss: 3.2646 - val_task1_loss: 1.6323 - val_task2_loss: 1.6323 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "3141/3143 [============================>.] - ETA: 0s - loss: 3.1422 - task1_loss: 1.5711 - task2_loss: 1.5711\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "3143/3143 [==============================] - 49s 16ms/step - loss: 3.1422 - task1_loss: 1.5711 - task2_loss: 1.5711 - val_loss: 3.2664 - val_task1_loss: 1.6332 - val_task2_loss: 1.6332 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "3143/3143 [==============================] - 50s 16ms/step - loss: 3.1258 - task1_loss: 1.5629 - task2_loss: 1.5629 - val_loss: 3.2684 - val_task1_loss: 1.6342 - val_task2_loss: 1.6342 - lr: 4.0000e-05\n",
      "Epoch 16/100\n",
      "3143/3143 [==============================] - 51s 16ms/step - loss: 3.1222 - task1_loss: 1.5611 - task2_loss: 1.5611 - val_loss: 3.2701 - val_task1_loss: 1.6350 - val_task2_loss: 1.6351 - lr: 4.0000e-05\n",
      "Epoch 17/100\n",
      "3141/3143 [============================>.] - ETA: 0s - loss: 3.1212 - task1_loss: 1.5606 - task2_loss: 1.5606\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "3143/3143 [==============================] - 50s 16ms/step - loss: 3.1212 - task1_loss: 1.5606 - task2_loss: 1.5606 - val_loss: 3.2707 - val_task1_loss: 1.6353 - val_task2_loss: 1.6354 - lr: 4.0000e-05\n",
      "Epoch 18/100\n",
      "3143/3143 [==============================] - 49s 16ms/step - loss: 3.1166 - task1_loss: 1.5583 - task2_loss: 1.5583 - val_loss: 3.2716 - val_task1_loss: 1.6358 - val_task2_loss: 1.6358 - lr: 8.0000e-06\n",
      "Epoch 19/100\n",
      "3143/3143 [==============================] - 50s 16ms/step - loss: 3.1155 - task1_loss: 1.5577 - task2_loss: 1.5577 - val_loss: 3.2718 - val_task1_loss: 1.6359 - val_task2_loss: 1.6359 - lr: 8.0000e-06\n",
      "Epoch 20/100\n",
      "3140/3143 [============================>.] - ETA: 0s - loss: 3.1148 - task1_loss: 1.5574 - task2_loss: 1.5574\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "3143/3143 [==============================] - 50s 16ms/step - loss: 3.1148 - task1_loss: 1.5574 - task2_loss: 1.5574 - val_loss: 3.2725 - val_task1_loss: 1.6362 - val_task2_loss: 1.6362 - lr: 8.0000e-06\n",
      "Epoch 21/100\n",
      "3141/3143 [============================>.] - ETA: 0s - loss: 3.1144 - task1_loss: 1.5572 - task2_loss: 1.5572Restoring model weights from the end of the best epoch: 11.\n",
      "3143/3143 [==============================] - 51s 16ms/step - loss: 3.1144 - task1_loss: 1.5572 - task2_loss: 1.5572 - val_loss: 3.2721 - val_task1_loss: 1.6361 - val_task2_loss: 1.6361 - lr: 1.6000e-06\n",
      "Epoch 21: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# tasks = ['Dev','Hk']\n",
    "\n",
    "inputs, outputs = DeepSTARR(input_shape=(L,A))\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    keras.optimizers.Adam(lr=0.001),\n",
    "    loss={'task1': laplace_nll_loss, 'task2': laplace_nll_loss},\n",
    "    loss_weights={'task1': 1.0, 'task2': 1.0},\n",
    "    # run_eagerly=True  # Enable eager execution for the model\n",
    ")\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', #'val_aupr',#\n",
    "                                            patience=10,\n",
    "                                            verbose=1,\n",
    "                                            mode='min',\n",
    "                                            restore_best_weights=True)\n",
    "# reduce learning rate callback\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.2,\n",
    "                                                patience=3,\n",
    "                                                min_lr=1e-7,\n",
    "                                                mode='min',\n",
    "                                                verbose=1)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[es_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8ad0f4-7481-46e6-9c82-97ad454c0d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 1s 10ms/step\n",
      " MSE Dev = 1.596\n",
      " PCC Dev = 0.585\n",
      " SCC Dev = 0.607\n",
      " MSE Hk = 1.594\n",
      " PCC Hk = 0.690\n",
      " SCC Hk = 0.553\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create functions\n",
    "def summary_statistics(pred, Y, task):\n",
    "    if task ==\"Dev\":\n",
    "        i=0\n",
    "    if task ==\"Hk\":\n",
    "        i=1\n",
    "    mse = mean_squared_error(Y[:,i], pred[:,0])\n",
    "    pcc = stats.pearsonr(Y[:,i], pred[:,0])[0]\n",
    "    scc = stats.spearmanr(Y[:,i], pred[:,0])[0]\n",
    "    print(' MSE ' + task + ' = ' + str(\"{0:0.3f}\".format(mse)))\n",
    "    print(' PCC ' + task + ' = ' + str(\"{0:0.3f}\".format(pcc)))\n",
    "    print(' SCC ' + task + ' = ' + str(\"{0:0.3f}\".format(scc)))\n",
    "\n",
    "# run for each set and enhancer type\n",
    "pred = model.predict(x_test, batch_size=512)\n",
    "results_dev = summary_statistics(pred[0],  y_test, \"Dev\")\n",
    "results_hk = summary_statistics(pred[1],  y_test, \"Hk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d424969e-8afb-46bf-983a-2d9f8fac991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mosaic/miniconda3/envs/tf2/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3143/3143 [==============================] - 53s 16ms/step - loss: 3.7795 - task1_loss: 1.8900 - task2_loss: 1.8895 - val_loss: 3.6851 - val_task1_loss: 1.8425 - val_task2_loss: 1.8425 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3143/3143 [==============================] - 49s 15ms/step - loss: 3.6274 - task1_loss: 1.8137 - task2_loss: 1.8137 - val_loss: 3.6512 - val_task1_loss: 1.8256 - val_task2_loss: 1.8256 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3143/3143 [==============================] - 52s 16ms/step - loss: 3.5833 - task1_loss: 1.7916 - task2_loss: 1.7916 - val_loss: 3.7229 - val_task1_loss: 1.8614 - val_task2_loss: 1.8614 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3143/3143 [==============================] - 52s 17ms/step - loss: 3.5544 - task1_loss: 1.7772 - task2_loss: 1.7772 - val_loss: 3.5950 - val_task1_loss: 1.7975 - val_task2_loss: 1.7975 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3143/3143 [==============================] - 52s 16ms/step - loss: 3.5309 - task1_loss: 1.7654 - task2_loss: 1.7654 - val_loss: 3.5255 - val_task1_loss: 1.7628 - val_task2_loss: 1.7628 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3143/3143 [==============================] - 53s 17ms/step - loss: 3.5130 - task1_loss: 1.7565 - task2_loss: 1.7565 - val_loss: 3.6059 - val_task1_loss: 1.8030 - val_task2_loss: 1.8030 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "3143/3143 [==============================] - 52s 16ms/step - loss: 3.4945 - task1_loss: 1.7473 - task2_loss: 1.7473 - val_loss: 3.5521 - val_task1_loss: 1.7760 - val_task2_loss: 1.7760 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "3143/3143 [==============================] - 53s 17ms/step - loss: 3.4801 - task1_loss: 1.7401 - task2_loss: 1.7401 - val_loss: 3.5024 - val_task1_loss: 1.7512 - val_task2_loss: 1.7512 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "3143/3143 [==============================] - 49s 16ms/step - loss: 3.4646 - task1_loss: 1.7323 - task2_loss: 1.7323 - val_loss: 3.5099 - val_task1_loss: 1.7550 - val_task2_loss: 1.7550 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "3143/3143 [==============================] - 52s 17ms/step - loss: 3.4512 - task1_loss: 1.7256 - task2_loss: 1.7256 - val_loss: 3.6508 - val_task1_loss: 1.8254 - val_task2_loss: 1.8254 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "3143/3143 [==============================] - ETA: 0s - loss: 3.4383 - task1_loss: 1.7192 - task2_loss: 1.7192\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "3143/3143 [==============================] - 51s 16ms/step - loss: 3.4383 - task1_loss: 1.7192 - task2_loss: 1.7192 - val_loss: 3.5099 - val_task1_loss: 1.7550 - val_task2_loss: 1.7550 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "3143/3143 [==============================] - 51s 16ms/step - loss: 3.3837 - task1_loss: 1.6919 - task2_loss: 1.6919 - val_loss: 3.5163 - val_task1_loss: 1.7582 - val_task2_loss: 1.7582 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "3143/3143 [==============================] - 53s 17ms/step - loss: 3.3650 - task1_loss: 1.6825 - task2_loss: 1.6825 - val_loss: 3.5256 - val_task1_loss: 1.7628 - val_task2_loss: 1.7628 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "3141/3143 [============================>.] - ETA: 0s - loss: 3.3552 - task1_loss: 1.6776 - task2_loss: 1.6776\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "3143/3143 [==============================] - 52s 17ms/step - loss: 3.3551 - task1_loss: 1.6775 - task2_loss: 1.6775 - val_loss: 3.5234 - val_task1_loss: 1.7617 - val_task2_loss: 1.7617 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "3143/3143 [==============================] - 52s 17ms/step - loss: 3.3354 - task1_loss: 1.6677 - task2_loss: 1.6677 - val_loss: 3.5284 - val_task1_loss: 1.7642 - val_task2_loss: 1.7642 - lr: 4.0000e-05\n",
      "Epoch 16/100\n",
      "3143/3143 [==============================] - 52s 17ms/step - loss: 3.3329 - task1_loss: 1.6664 - task2_loss: 1.6664 - val_loss: 3.5306 - val_task1_loss: 1.7653 - val_task2_loss: 1.7653 - lr: 4.0000e-05\n",
      "Epoch 17/100\n",
      "3142/3143 [============================>.] - ETA: 0s - loss: 3.3307 - task1_loss: 1.6653 - task2_loss: 1.6653\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "3143/3143 [==============================] - 52s 17ms/step - loss: 3.3307 - task1_loss: 1.6653 - task2_loss: 1.6653 - val_loss: 3.5331 - val_task1_loss: 1.7666 - val_task2_loss: 1.7666 - lr: 4.0000e-05\n",
      "Epoch 18/100\n",
      "3141/3143 [============================>.] - ETA: 0s - loss: 3.3256 - task1_loss: 1.6628 - task2_loss: 1.6628Restoring model weights from the end of the best epoch: 8.\n",
      "3143/3143 [==============================] - 52s 17ms/step - loss: 3.3255 - task1_loss: 1.6628 - task2_loss: 1.6628 - val_loss: 3.5328 - val_task1_loss: 1.7664 - val_task2_loss: 1.7664 - lr: 8.0000e-06\n",
      "Epoch 18: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# tasks = ['Dev','Hk']\n",
    "\n",
    "inputs, outputs = DeepSTARR(input_shape=(L,A))\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    keras.optimizers.Adam(lr=0.002),\n",
    "    loss={'task1': cauchy_nll_loss, 'task2': cauchy_nll_loss},\n",
    "    loss_weights={'task1': 1.0, 'task2': 1.0},\n",
    "    # run_eagerly=True  # Enable eager execution for the model\n",
    ")\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', #'val_aupr',#\n",
    "                                            patience=10,\n",
    "                                            verbose=1,\n",
    "                                            mode='min',\n",
    "                                            restore_best_weights=True)\n",
    "# reduce learning rate callback\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.2,\n",
    "                                                patience=3,\n",
    "                                                min_lr=1e-7,\n",
    "                                                mode='min',\n",
    "                                                verbose=1)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[es_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18777e85-57e1-4293-a019-4c4904b56d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mosaic/miniconda3/envs/tf2/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.7801 - task1_loss: 1.8883 - task2_loss: 1.8918 - val_loss: 3.6420 - val_task1_loss: 1.8210 - val_task2_loss: 1.8210 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.6290 - task1_loss: 1.8145 - task2_loss: 1.8145 - val_loss: 3.5814 - val_task1_loss: 1.7907 - val_task2_loss: 1.7907 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.5860 - task1_loss: 1.7930 - task2_loss: 1.7930 - val_loss: 3.5637 - val_task1_loss: 1.7819 - val_task2_loss: 1.7819 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3143/3143 [==============================] - 39s 13ms/step - loss: 3.5575 - task1_loss: 1.7787 - task2_loss: 1.7787 - val_loss: 3.6588 - val_task1_loss: 1.8294 - val_task2_loss: 1.8294 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.5332 - task1_loss: 1.7666 - task2_loss: 1.7666 - val_loss: 3.5759 - val_task1_loss: 1.7880 - val_task2_loss: 1.7880 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.5149 - task1_loss: 1.7574 - task2_loss: 1.7574 - val_loss: 3.5505 - val_task1_loss: 1.7753 - val_task2_loss: 1.7753 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.4979 - task1_loss: 1.7490 - task2_loss: 1.7490 - val_loss: 3.5212 - val_task1_loss: 1.7606 - val_task2_loss: 1.7606 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.4825 - task1_loss: 1.7412 - task2_loss: 1.7412 - val_loss: 3.5079 - val_task1_loss: 1.7539 - val_task2_loss: 1.7539 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.4675 - task1_loss: 1.7337 - task2_loss: 1.7337 - val_loss: 3.5147 - val_task1_loss: 1.7574 - val_task2_loss: 1.7574 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.4523 - task1_loss: 1.7261 - task2_loss: 1.7261 - val_loss: 3.5389 - val_task1_loss: 1.7694 - val_task2_loss: 1.7694 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "3143/3143 [==============================] - ETA: 0s - loss: 3.4380 - task1_loss: 1.7190 - task2_loss: 1.7190\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.4380 - task1_loss: 1.7190 - task2_loss: 1.7190 - val_loss: 3.5208 - val_task1_loss: 1.7604 - val_task2_loss: 1.7604 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3855 - task1_loss: 1.6927 - task2_loss: 1.6927 - val_loss: 3.5212 - val_task1_loss: 1.7606 - val_task2_loss: 1.7606 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3661 - task1_loss: 1.6831 - task2_loss: 1.6831 - val_loss: 3.5257 - val_task1_loss: 1.7629 - val_task2_loss: 1.7629 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "3140/3143 [============================>.] - ETA: 0s - loss: 3.3558 - task1_loss: 1.6779 - task2_loss: 1.6779\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3559 - task1_loss: 1.6779 - task2_loss: 1.6779 - val_loss: 3.5306 - val_task1_loss: 1.7653 - val_task2_loss: 1.7653 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.3374 - task1_loss: 1.6687 - task2_loss: 1.6687 - val_loss: 3.5358 - val_task1_loss: 1.7679 - val_task2_loss: 1.7679 - lr: 4.0000e-05\n",
      "Epoch 16/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.3337 - task1_loss: 1.6669 - task2_loss: 1.6669 - val_loss: 3.5383 - val_task1_loss: 1.7691 - val_task2_loss: 1.7691 - lr: 4.0000e-05\n",
      "Epoch 17/100\n",
      "3140/3143 [============================>.] - ETA: 0s - loss: 3.3318 - task1_loss: 1.6659 - task2_loss: 1.6659\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.3319 - task1_loss: 1.6659 - task2_loss: 1.6659 - val_loss: 3.5395 - val_task1_loss: 1.7697 - val_task2_loss: 1.7697 - lr: 4.0000e-05\n",
      "Epoch 18/100\n",
      "3139/3143 [============================>.] - ETA: 0s - loss: 3.3274 - task1_loss: 1.6637 - task2_loss: 1.6637Restoring model weights from the end of the best epoch: 8.\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3273 - task1_loss: 1.6637 - task2_loss: 1.6637 - val_loss: 3.5410 - val_task1_loss: 1.7705 - val_task2_loss: 1.7705 - lr: 8.0000e-06\n",
      "Epoch 18: early stopping\n",
      "81/81 [==============================] - 1s 6ms/step\n",
      " MSE Dev = 1.772\n",
      " PCC Dev = 0.552\n",
      " SCC Dev = 0.590\n",
      " MSE Hk = 1.557\n",
      " PCC Hk = 0.690\n",
      " SCC Hk = 0.545\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# tasks = ['Dev','Hk']\n",
    "\n",
    "inputs, outputs = DeepSTARR(input_shape=(L,A))\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    keras.optimizers.Adam(lr=0.001),\n",
    "    loss={'task1': cauchy_nll_loss, 'task2': cauchy_nll_loss},\n",
    "    loss_weights={'task1': 1.0, 'task2': 1.0},\n",
    "    # run_eagerly=True  # Enable eager execution for the model\n",
    ")\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', #'val_aupr',#\n",
    "                                            patience=10,\n",
    "                                            verbose=1,\n",
    "                                            mode='min',\n",
    "                                            restore_best_weights=True)\n",
    "# reduce learning rate callback\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.2,\n",
    "                                                patience=3,\n",
    "                                                min_lr=1e-7,\n",
    "                                                mode='min',\n",
    "                                                verbose=1)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[es_callback, reduce_lr])\n",
    "\n",
    "\n",
    "# run for each set and enhancer type\n",
    "pred = model.predict(x_test, batch_size=512)\n",
    "results_dev = summary_statistics(pred[0],  y_test, \"Dev\")\n",
    "results_hk = summary_statistics(pred[1],  y_test, \"Hk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85a63b94-df5a-4c2a-9c2b-0e8e3bf21389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.7452 - task1_loss: 1.8726 - task2_loss: 1.8727 - val_loss: 3.6267 - val_task1_loss: 1.8133 - val_task2_loss: 1.8134 - lr: 0.0020\n",
      "Epoch 2/100\n",
      "3143/3143 [==============================] - 38s 12ms/step - loss: 3.6157 - task1_loss: 1.8079 - task2_loss: 1.8079 - val_loss: 3.5704 - val_task1_loss: 1.7852 - val_task2_loss: 1.7852 - lr: 0.0020\n",
      "Epoch 3/100\n",
      "3143/3143 [==============================] - 39s 13ms/step - loss: 3.5752 - task1_loss: 1.7876 - task2_loss: 1.7876 - val_loss: 3.5941 - val_task1_loss: 1.7970 - val_task2_loss: 1.7970 - lr: 0.0020\n",
      "Epoch 4/100\n",
      "3143/3143 [==============================] - 39s 13ms/step - loss: 3.5448 - task1_loss: 1.7724 - task2_loss: 1.7724 - val_loss: 3.5394 - val_task1_loss: 1.7697 - val_task2_loss: 1.7697 - lr: 0.0020\n",
      "Epoch 5/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.5235 - task1_loss: 1.7618 - task2_loss: 1.7618 - val_loss: 3.4999 - val_task1_loss: 1.7500 - val_task2_loss: 1.7500 - lr: 0.0020\n",
      "Epoch 6/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.5049 - task1_loss: 1.7524 - task2_loss: 1.7524 - val_loss: 3.5324 - val_task1_loss: 1.7662 - val_task2_loss: 1.7662 - lr: 0.0020\n",
      "Epoch 7/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.4882 - task1_loss: 1.7441 - task2_loss: 1.7441 - val_loss: 3.4912 - val_task1_loss: 1.7456 - val_task2_loss: 1.7456 - lr: 0.0020\n",
      "Epoch 8/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.4723 - task1_loss: 1.7361 - task2_loss: 1.7361 - val_loss: 3.5288 - val_task1_loss: 1.7644 - val_task2_loss: 1.7644 - lr: 0.0020\n",
      "Epoch 9/100\n",
      "3143/3143 [==============================] - 39s 13ms/step - loss: 3.4601 - task1_loss: 1.7301 - task2_loss: 1.7301 - val_loss: 3.5348 - val_task1_loss: 1.7674 - val_task2_loss: 1.7674 - lr: 0.0020\n",
      "Epoch 10/100\n",
      "3143/3143 [==============================] - ETA: 0s - loss: 3.4455 - task1_loss: 1.7227 - task2_loss: 1.7227\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.4455 - task1_loss: 1.7227 - task2_loss: 1.7227 - val_loss: 3.5041 - val_task1_loss: 1.7521 - val_task2_loss: 1.7521 - lr: 0.0020\n",
      "Epoch 11/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3905 - task1_loss: 1.6952 - task2_loss: 1.6952 - val_loss: 3.5002 - val_task1_loss: 1.7501 - val_task2_loss: 1.7501 - lr: 4.0000e-04\n",
      "Epoch 12/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3711 - task1_loss: 1.6855 - task2_loss: 1.6855 - val_loss: 3.5080 - val_task1_loss: 1.7540 - val_task2_loss: 1.7540 - lr: 4.0000e-04\n",
      "Epoch 13/100\n",
      "3143/3143 [==============================] - ETA: 0s - loss: 3.3589 - task1_loss: 1.6794 - task2_loss: 1.6794\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.3589 - task1_loss: 1.6794 - task2_loss: 1.6794 - val_loss: 3.5135 - val_task1_loss: 1.7567 - val_task2_loss: 1.7567 - lr: 4.0000e-04\n",
      "Epoch 14/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3401 - task1_loss: 1.6700 - task2_loss: 1.6700 - val_loss: 3.5163 - val_task1_loss: 1.7581 - val_task2_loss: 1.7581 - lr: 8.0000e-05\n",
      "Epoch 15/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.3360 - task1_loss: 1.6680 - task2_loss: 1.6680 - val_loss: 3.5189 - val_task1_loss: 1.7594 - val_task2_loss: 1.7594 - lr: 8.0000e-05\n",
      "Epoch 16/100\n",
      "3143/3143 [==============================] - ETA: 0s - loss: 3.3320 - task1_loss: 1.6660 - task2_loss: 1.6660\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.3320 - task1_loss: 1.6660 - task2_loss: 1.6660 - val_loss: 3.5226 - val_task1_loss: 1.7613 - val_task2_loss: 1.7613 - lr: 8.0000e-05\n",
      "Epoch 17/100\n",
      "3143/3143 [==============================] - ETA: 0s - loss: 3.3286 - task1_loss: 1.6643 - task2_loss: 1.6643Restoring model weights from the end of the best epoch: 7.\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3286 - task1_loss: 1.6643 - task2_loss: 1.6643 - val_loss: 3.5225 - val_task1_loss: 1.7613 - val_task2_loss: 1.7613 - lr: 1.6000e-05\n",
      "Epoch 17: early stopping\n",
      "81/81 [==============================] - 1s 7ms/step\n",
      " MSE Dev = 1.668\n",
      " PCC Dev = 0.577\n",
      " SCC Dev = 0.599\n",
      " MSE Hk = 1.554\n",
      " PCC Hk = 0.691\n",
      " SCC Hk = 0.554\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# tasks = ['Dev','Hk']\n",
    "\n",
    "inputs, outputs = DeepSTARR(input_shape=(L,A))\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    keras.optimizers.Adam(lr=0.001),\n",
    "    loss={'task1': cauchy_nll_loss, 'task2': cauchy_nll_loss},\n",
    "    loss_weights={'task1': 1.0, 'task2': 1.0},\n",
    "    # run_eagerly=True  # Enable eager execution for the model\n",
    ")\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', #'val_aupr',#\n",
    "                                            patience=10,\n",
    "                                            verbose=1,\n",
    "                                            mode='min',\n",
    "                                            restore_best_weights=True)\n",
    "# reduce learning rate callback\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.2,\n",
    "                                                patience=3,\n",
    "                                                min_lr=1e-7,\n",
    "                                                mode='min',\n",
    "                                                verbose=1)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[es_callback, reduce_lr])\n",
    "\n",
    "\n",
    "# run for each set and enhancer type\n",
    "pred = model.predict(x_test, batch_size=512)\n",
    "results_dev = summary_statistics(pred[0],  y_test, \"Dev\")\n",
    "results_hk = summary_statistics(pred[1],  y_test, \"Hk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21026482-f6f6-4778-bab8-84d376056550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mosaic/miniconda3/envs/tf2/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3143/3143 [==============================] - 42s 13ms/step - loss: 3.7779 - task1_loss: 1.8874 - task2_loss: 1.8905 - val_loss: 3.7825 - val_task1_loss: 1.8913 - val_task2_loss: 1.8911 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3143/3143 [==============================] - 39s 13ms/step - loss: 3.6309 - task1_loss: 1.8154 - task2_loss: 1.8154 - val_loss: 3.8395 - val_task1_loss: 1.9198 - val_task2_loss: 1.9198 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.5853 - task1_loss: 1.7926 - task2_loss: 1.7926 - val_loss: 3.6133 - val_task1_loss: 1.8066 - val_task2_loss: 1.8066 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.5554 - task1_loss: 1.7777 - task2_loss: 1.7777 - val_loss: 3.5212 - val_task1_loss: 1.7606 - val_task2_loss: 1.7606 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.5331 - task1_loss: 1.7665 - task2_loss: 1.7665 - val_loss: 3.5118 - val_task1_loss: 1.7559 - val_task2_loss: 1.7559 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.5140 - task1_loss: 1.7570 - task2_loss: 1.7570 - val_loss: 3.5296 - val_task1_loss: 1.7648 - val_task2_loss: 1.7648 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.4959 - task1_loss: 1.7480 - task2_loss: 1.7480 - val_loss: 3.5701 - val_task1_loss: 1.7850 - val_task2_loss: 1.7850 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.4813 - task1_loss: 1.7406 - task2_loss: 1.7406 - val_loss: 3.5065 - val_task1_loss: 1.7532 - val_task2_loss: 1.7532 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.4658 - task1_loss: 1.7329 - task2_loss: 1.7329 - val_loss: 3.5468 - val_task1_loss: 1.7734 - val_task2_loss: 1.7734 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "3143/3143 [==============================] - 38s 12ms/step - loss: 3.4524 - task1_loss: 1.7262 - task2_loss: 1.7262 - val_loss: 3.5135 - val_task1_loss: 1.7568 - val_task2_loss: 1.7568 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "3141/3143 [============================>.] - ETA: 0s - loss: 3.4390 - task1_loss: 1.7195 - task2_loss: 1.7195\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.4390 - task1_loss: 1.7195 - task2_loss: 1.7195 - val_loss: 3.5077 - val_task1_loss: 1.7538 - val_task2_loss: 1.7538 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3855 - task1_loss: 1.6928 - task2_loss: 1.6928 - val_loss: 3.5130 - val_task1_loss: 1.7565 - val_task2_loss: 1.7565 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3664 - task1_loss: 1.6832 - task2_loss: 1.6832 - val_loss: 3.5201 - val_task1_loss: 1.7600 - val_task2_loss: 1.7600 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "3140/3143 [============================>.] - ETA: 0s - loss: 3.3565 - task1_loss: 1.6782 - task2_loss: 1.6782\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.3565 - task1_loss: 1.6783 - task2_loss: 1.6783 - val_loss: 3.5239 - val_task1_loss: 1.7620 - val_task2_loss: 1.7620 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3368 - task1_loss: 1.6684 - task2_loss: 1.6684 - val_loss: 3.5282 - val_task1_loss: 1.7641 - val_task2_loss: 1.7641 - lr: 4.0000e-05\n",
      "Epoch 16/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3330 - task1_loss: 1.6665 - task2_loss: 1.6665 - val_loss: 3.5316 - val_task1_loss: 1.7658 - val_task2_loss: 1.7658 - lr: 4.0000e-05\n",
      "Epoch 17/100\n",
      "3141/3143 [============================>.] - ETA: 0s - loss: 3.3302 - task1_loss: 1.6651 - task2_loss: 1.6651\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3303 - task1_loss: 1.6651 - task2_loss: 1.6651 - val_loss: 3.5340 - val_task1_loss: 1.7670 - val_task2_loss: 1.7670 - lr: 4.0000e-05\n",
      "Epoch 18/100\n",
      "3143/3143 [==============================] - ETA: 0s - loss: 3.3267 - task1_loss: 1.6634 - task2_loss: 1.6634Restoring model weights from the end of the best epoch: 8.\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3267 - task1_loss: 1.6634 - task2_loss: 1.6634 - val_loss: 3.5337 - val_task1_loss: 1.7668 - val_task2_loss: 1.7668 - lr: 8.0000e-06\n",
      "Epoch 18: early stopping\n",
      "81/81 [==============================] - 1s 6ms/step\n",
      " MSE Dev = 1.775\n",
      " PCC Dev = 0.539\n",
      " SCC Dev = 0.579\n",
      " MSE Hk = 1.510\n",
      " PCC Hk = 0.707\n",
      " SCC Hk = 0.557\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# tasks = ['Dev','Hk']\n",
    "\n",
    "inputs, outputs = DeepSTARR(input_shape=(L,A))\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    keras.optimizers.Adam(lr=0.001),\n",
    "    loss={'task1': cauchy_nll_loss, 'task2': cauchy_nll_loss},\n",
    "    loss_weights={'task1': 1.0, 'task2': 1.0},\n",
    "    # run_eagerly=True  # Enable eager execution for the model\n",
    ")\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', #'val_aupr',#\n",
    "                                            patience=10,\n",
    "                                            verbose=1,\n",
    "                                            mode='min',\n",
    "                                            restore_best_weights=True)\n",
    "# reduce learning rate callback\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.2,\n",
    "                                                patience=3,\n",
    "                                                min_lr=1e-7,\n",
    "                                                mode='min',\n",
    "                                                verbose=1)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[es_callback, reduce_lr])\n",
    "\n",
    "\n",
    "# run for each set and enhancer type\n",
    "pred = model.predict(x_test, batch_size=512)\n",
    "results_dev = summary_statistics(pred[0],  y_test, \"Dev\")\n",
    "results_hk = summary_statistics(pred[1],  y_test, \"Hk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa1e0b7c-ee84-47d4-96e7-1ffb85346e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 1s 9ms/step\n",
      " MSE Dev = 1.757\n",
      " PCC Dev = 0.552\n",
      " SCC Dev = 0.588\n",
      " MSE Hk = 1.555\n",
      " PCC Hk = 0.692\n",
      " SCC Hk = 0.546\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run for each set and enhancer type\n",
    "pred = model.predict(x_test, batch_size=512)\n",
    "results_dev = summary_statistics(pred[0],  y_test, \"Dev\")\n",
    "results_hk = summary_statistics(pred[1],  y_test, \"Hk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0baca6be-1fcc-4cdd-bf79-d18cb1da3fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:13:02.383033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-27 13:13:02.401337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-27 13:13:02.401511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-27 13:13:02.402168: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 13:13:02.403095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-27 13:13:02.403256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-27 13:13:02.403360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-27 13:13:02.900434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-27 13:13:02.900797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-27 13:13:02.900913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-27 13:13:02.901017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14021 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/mosaic/miniconda3/envs/tf2/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 13:13:05.744209: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2024-05-27 13:13:06.457259: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-27 13:13:06.457749: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-27 13:13:06.457759: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-05-27 13:13:06.458092: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-27 13:13:06.458121: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-05-27 13:13:07.019469: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3143/3143 [==============================] - 41s 12ms/step - loss: 3.7391 - task1_loss: 1.9013 - task2_loss: 1.8377 - val_loss: 3.4736 - val_task1_loss: 1.7405 - val_task2_loss: 1.7331 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.4585 - task1_loss: 1.7294 - task2_loss: 1.7291 - val_loss: 3.5500 - val_task1_loss: 1.7769 - val_task2_loss: 1.7732 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3143/3143 [==============================] - 38s 12ms/step - loss: 3.4001 - task1_loss: 1.7002 - task2_loss: 1.6999 - val_loss: 3.5539 - val_task1_loss: 1.7773 - val_task2_loss: 1.7766 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3143/3143 [==============================] - 38s 12ms/step - loss: 3.3604 - task1_loss: 1.6802 - task2_loss: 1.6802 - val_loss: 3.4397 - val_task1_loss: 1.7199 - val_task2_loss: 1.7198 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.3304 - task1_loss: 1.6652 - task2_loss: 1.6652 - val_loss: 3.3323 - val_task1_loss: 1.6662 - val_task2_loss: 1.6662 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3039 - task1_loss: 1.6519 - task2_loss: 1.6519 - val_loss: 3.4364 - val_task1_loss: 1.7182 - val_task2_loss: 1.7182 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "3143/3143 [==============================] - 38s 12ms/step - loss: 3.2836 - task1_loss: 1.6418 - task2_loss: 1.6418 - val_loss: 3.5536 - val_task1_loss: 1.7768 - val_task2_loss: 1.7768 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.2637 - task1_loss: 1.6319 - task2_loss: 1.6319 - val_loss: 3.3256 - val_task1_loss: 1.6628 - val_task2_loss: 1.6628 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "3143/3143 [==============================] - 37s 12ms/step - loss: 3.2464 - task1_loss: 1.6232 - task2_loss: 1.6232 - val_loss: 3.3449 - val_task1_loss: 1.6724 - val_task2_loss: 1.6724 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "3143/3143 [==============================] - 39s 13ms/step - loss: 3.2318 - task1_loss: 1.6159 - task2_loss: 1.6159 - val_loss: 3.4071 - val_task1_loss: 1.7035 - val_task2_loss: 1.7035 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.2175 - task1_loss: 1.6087 - task2_loss: 1.6087 - val_loss: 3.3201 - val_task1_loss: 1.6601 - val_task2_loss: 1.6601 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "3143/3143 [==============================] - 38s 12ms/step - loss: 3.2042 - task1_loss: 1.6021 - task2_loss: 1.6021 - val_loss: 3.3245 - val_task1_loss: 1.6623 - val_task2_loss: 1.6622 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "3143/3143 [==============================] - 39s 13ms/step - loss: 3.1940 - task1_loss: 1.5970 - task2_loss: 1.5970 - val_loss: 3.4496 - val_task1_loss: 1.7248 - val_task2_loss: 1.7248 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "3143/3143 [==============================] - ETA: 0s - loss: 3.1821 - task1_loss: 1.5911 - task2_loss: 1.5911\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.1821 - task1_loss: 1.5911 - task2_loss: 1.5911 - val_loss: 3.3481 - val_task1_loss: 1.6740 - val_task2_loss: 1.6740 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.1318 - task1_loss: 1.5659 - task2_loss: 1.5659 - val_loss: 3.3374 - val_task1_loss: 1.6687 - val_task2_loss: 1.6687 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.1153 - task1_loss: 1.5576 - task2_loss: 1.5576 - val_loss: 3.3443 - val_task1_loss: 1.6721 - val_task2_loss: 1.6721 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "3141/3143 [============================>.] - ETA: 0s - loss: 3.1057 - task1_loss: 1.5529 - task2_loss: 1.5529\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "3143/3143 [==============================] - 39s 13ms/step - loss: 3.1057 - task1_loss: 1.5528 - task2_loss: 1.5528 - val_loss: 3.3718 - val_task1_loss: 1.6859 - val_task2_loss: 1.6859 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "3143/3143 [==============================] - 38s 12ms/step - loss: 3.0896 - task1_loss: 1.5448 - task2_loss: 1.5448 - val_loss: 3.3622 - val_task1_loss: 1.6811 - val_task2_loss: 1.6811 - lr: 4.0000e-05\n",
      "Epoch 19/100\n",
      "3143/3143 [==============================] - 38s 12ms/step - loss: 3.0875 - task1_loss: 1.5438 - task2_loss: 1.5438 - val_loss: 3.3621 - val_task1_loss: 1.6811 - val_task2_loss: 1.6811 - lr: 4.0000e-05\n",
      "Epoch 20/100\n",
      "3141/3143 [============================>.] - ETA: 0s - loss: 3.0857 - task1_loss: 1.5428 - task2_loss: 1.5428\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "3143/3143 [==============================] - 39s 13ms/step - loss: 3.0857 - task1_loss: 1.5429 - task2_loss: 1.5429 - val_loss: 3.3643 - val_task1_loss: 1.6821 - val_task2_loss: 1.6821 - lr: 4.0000e-05\n",
      "Epoch 21/100\n",
      "3143/3143 [==============================] - ETA: 0s - loss: 3.0822 - task1_loss: 1.5411 - task2_loss: 1.5411Restoring model weights from the end of the best epoch: 11.\n",
      "3143/3143 [==============================] - 38s 12ms/step - loss: 3.0822 - task1_loss: 1.5411 - task2_loss: 1.5411 - val_loss: 3.3659 - val_task1_loss: 1.6829 - val_task2_loss: 1.6829 - lr: 8.0000e-06\n",
      "Epoch 21: early stopping\n",
      "81/81 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'summary_statistics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# run for each set and enhancer type\u001b[39;00m\n\u001b[1;32m     42\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m results_dev \u001b[38;5;241m=\u001b[39m \u001b[43msummary_statistics\u001b[49m(pred[\u001b[38;5;241m0\u001b[39m],  y_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m results_hk \u001b[38;5;241m=\u001b[39m summary_statistics(pred[\u001b[38;5;241m1\u001b[39m],  y_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_statistics' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# tasks = ['Dev','Hk']\n",
    "\n",
    "inputs, outputs = DeepSTARR(input_shape=(L,A))\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    keras.optimizers.Adam(lr=0.002),\n",
    "    loss={'task1': gaussian_nll_loss, 'task2': gaussian_nll_loss},\n",
    "    loss_weights={'task1': 1.0, 'task2': 1.0},\n",
    "    # run_eagerly=True  # Enable eager execution for the model\n",
    ")\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', #'val_aupr',#\n",
    "                                            patience=10,\n",
    "                                            verbose=1,\n",
    "                                            mode='min',\n",
    "                                            restore_best_weights=True)\n",
    "# reduce learning rate callback\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.2,\n",
    "                                                patience=3,\n",
    "                                                min_lr=1e-7,\n",
    "                                                mode='min',\n",
    "                                                verbose=1)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[es_callback, reduce_lr])\n",
    "\n",
    "\n",
    "# run for each set and enhancer type\n",
    "pred = model.predict(x_test, batch_size=512)\n",
    "results_dev = summary_statistics(pred[0],  y_test, \"Dev\")\n",
    "results_hk = summary_statistics(pred[1],  y_test, \"Hk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93fb1ca0-a000-4785-a22d-ec391dbc3ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 6ms/step\n",
      " MSE Dev = 1.636\n",
      " PCC Dev = 0.577\n",
      " SCC Dev = 0.592\n",
      " MSE Hk = 1.623\n",
      " PCC Hk = 0.678\n",
      " SCC Hk = 0.544\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create functions\n",
    "def summary_statistics(pred, Y, task):\n",
    "    if task ==\"Dev\":\n",
    "        i=0\n",
    "    if task ==\"Hk\":\n",
    "        i=1\n",
    "    mse = mean_squared_error(Y[:,i], pred[:,0])\n",
    "    pcc = stats.pearsonr(Y[:,i], pred[:,0])[0]\n",
    "    scc = stats.spearmanr(Y[:,i], pred[:,0])[0]\n",
    "    print(' MSE ' + task + ' = ' + str(\"{0:0.3f}\".format(mse)))\n",
    "    print(' PCC ' + task + ' = ' + str(\"{0:0.3f}\".format(pcc)))\n",
    "    print(' SCC ' + task + ' = ' + str(\"{0:0.3f}\".format(scc)))\n",
    "\n",
    "# run for each set and enhancer type\n",
    "pred = model.predict(x_test, batch_size=512)\n",
    "results_dev = summary_statistics(pred[0],  y_test, \"Dev\")\n",
    "results_hk = summary_statistics(pred[1],  y_test, \"Hk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e482b58-50ed-4ddb-b6ef-45d77e0e298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mosaic/miniconda3/envs/tf2/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.6674 - task1_loss: 1.8300 - task2_loss: 1.8374 - val_loss: 3.8270 - val_task1_loss: 1.9135 - val_task2_loss: 1.9136 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.4395 - task1_loss: 1.7197 - task2_loss: 1.7198 - val_loss: 3.4118 - val_task1_loss: 1.7059 - val_task2_loss: 1.7058 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3814 - task1_loss: 1.6907 - task2_loss: 1.6907 - val_loss: 3.4163 - val_task1_loss: 1.7081 - val_task2_loss: 1.7082 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3434 - task1_loss: 1.6717 - task2_loss: 1.6717 - val_loss: 3.3799 - val_task1_loss: 1.6900 - val_task2_loss: 1.6900 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.3147 - task1_loss: 1.6574 - task2_loss: 1.6574 - val_loss: 3.3333 - val_task1_loss: 1.6666 - val_task2_loss: 1.6666 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.2925 - task1_loss: 1.6462 - task2_loss: 1.6462 - val_loss: 3.3700 - val_task1_loss: 1.6850 - val_task2_loss: 1.6850 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.2719 - task1_loss: 1.6360 - task2_loss: 1.6360 - val_loss: 3.3245 - val_task1_loss: 1.6622 - val_task2_loss: 1.6622 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.2537 - task1_loss: 1.6269 - task2_loss: 1.6269 - val_loss: 3.3815 - val_task1_loss: 1.6907 - val_task2_loss: 1.6907 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.2374 - task1_loss: 1.6187 - task2_loss: 1.6187 - val_loss: 3.3239 - val_task1_loss: 1.6619 - val_task2_loss: 1.6619 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.2238 - task1_loss: 1.6119 - task2_loss: 1.6119 - val_loss: 3.3201 - val_task1_loss: 1.6601 - val_task2_loss: 1.6601 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.2088 - task1_loss: 1.6044 - task2_loss: 1.6044 - val_loss: 3.3346 - val_task1_loss: 1.6673 - val_task2_loss: 1.6673 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.1971 - task1_loss: 1.5986 - task2_loss: 1.5986 - val_loss: 3.3351 - val_task1_loss: 1.6675 - val_task2_loss: 1.6675 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "3140/3143 [============================>.] - ETA: 0s - loss: 3.1851 - task1_loss: 1.5926 - task2_loss: 1.5926\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "3143/3143 [==============================] - 42s 13ms/step - loss: 3.1851 - task1_loss: 1.5925 - task2_loss: 1.5925 - val_loss: 3.3475 - val_task1_loss: 1.6737 - val_task2_loss: 1.6737 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.1354 - task1_loss: 1.5677 - task2_loss: 1.5677 - val_loss: 3.3387 - val_task1_loss: 1.6694 - val_task2_loss: 1.6694 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "3143/3143 [==============================] - 42s 13ms/step - loss: 3.1189 - task1_loss: 1.5595 - task2_loss: 1.5595 - val_loss: 3.3435 - val_task1_loss: 1.6717 - val_task2_loss: 1.6717 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "3140/3143 [============================>.] - ETA: 0s - loss: 3.1093 - task1_loss: 1.5546 - task2_loss: 1.5546\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "3143/3143 [==============================] - 39s 12ms/step - loss: 3.1093 - task1_loss: 1.5546 - task2_loss: 1.5546 - val_loss: 3.3527 - val_task1_loss: 1.6763 - val_task2_loss: 1.6763 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.0932 - task1_loss: 1.5466 - task2_loss: 1.5466 - val_loss: 3.3523 - val_task1_loss: 1.6761 - val_task2_loss: 1.6761 - lr: 4.0000e-05\n",
      "Epoch 18/100\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.0916 - task1_loss: 1.5458 - task2_loss: 1.5458 - val_loss: 3.3531 - val_task1_loss: 1.6766 - val_task2_loss: 1.6766 - lr: 4.0000e-05\n",
      "Epoch 19/100\n",
      "3142/3143 [============================>.] - ETA: 0s - loss: 3.0889 - task1_loss: 1.5445 - task2_loss: 1.5445\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "3143/3143 [==============================] - 41s 13ms/step - loss: 3.0889 - task1_loss: 1.5444 - task2_loss: 1.5444 - val_loss: 3.3510 - val_task1_loss: 1.6755 - val_task2_loss: 1.6755 - lr: 4.0000e-05\n",
      "Epoch 20/100\n",
      "3139/3143 [============================>.] - ETA: 0s - loss: 3.0854 - task1_loss: 1.5427 - task2_loss: 1.5427Restoring model weights from the end of the best epoch: 10.\n",
      "3143/3143 [==============================] - 40s 13ms/step - loss: 3.0855 - task1_loss: 1.5427 - task2_loss: 1.5427 - val_loss: 3.3565 - val_task1_loss: 1.6782 - val_task2_loss: 1.6782 - lr: 8.0000e-06\n",
      "Epoch 20: early stopping\n",
      "81/81 [==============================] - 1s 6ms/step\n",
      " MSE Dev = 1.753\n",
      " PCC Dev = 0.560\n",
      " SCC Dev = 0.589\n",
      " MSE Hk = 1.499\n",
      " PCC Hk = 0.702\n",
      " SCC Hk = 0.547\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# tasks = ['Dev','Hk']\n",
    "\n",
    "inputs, outputs = DeepSTARR(input_shape=(L,A))\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    keras.optimizers.Adam(lr=0.001),\n",
    "    loss={'task1': gaussian_nll_loss, 'task2': gaussian_nll_loss},\n",
    "    loss_weights={'task1': 1.0, 'task2': 1.0},\n",
    "    # run_eagerly=True  # Enable eager execution for the model\n",
    ")\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', #'val_aupr',#\n",
    "                                            patience=10,\n",
    "                                            verbose=1,\n",
    "                                            mode='min',\n",
    "                                            restore_best_weights=True)\n",
    "# reduce learning rate callback\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                factor=0.2,\n",
    "                                                patience=3,\n",
    "                                                min_lr=1e-7,\n",
    "                                                mode='min',\n",
    "                                                verbose=1)\n",
    "\n",
    "# train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[es_callback, reduce_lr])\n",
    "\n",
    "\n",
    "# run for each set and enhancer type\n",
    "pred = model.predict(x_test, batch_size=512)\n",
    "results_dev = summary_statistics(pred[0],  y_test, \"Dev\")\n",
    "results_hk = summary_statistics(pred[1],  y_test, \"Hk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c502fb-34ca-47f3-aab5-ed666ef3524f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
